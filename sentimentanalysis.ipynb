{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597113775859",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Find reliable news with correspondance to stock prices\n",
    "# 2) Scrape news headlines+subheadlines\n",
    "# 3) Pre-processing and data cleaning\n",
    "# 4) Create/import dictionary\n",
    "# 5) Break news headlines/subheadlines into tokens\n",
    "# 6) Analyze + calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().date()\n",
    "def normalize_date(date):\n",
    "    normalize = str(date).split('-')\n",
    "    for i in normalize:\n",
    "        normalize = normalize[::-1]\n",
    "    normalize[1],normalize[0] = normalize[0],normalize[1]\n",
    "    new_date = '/'.join(normalize)\n",
    "    return str(new_date)\n",
    "\n",
    "\n",
    "# mm/dd/yy format\n",
    "googlenews = GoogleNews()\n",
    "googlenews.setlang('en')\n",
    "\n",
    "# Although increasing the timerange would seem logical since more data would mean a better fit, the overall sentiment became neutralised and did not provide meaningful results. Therefore, instead of having a large timerange, a shorter timerange (with less data as a tradeoff) was chosen to reflect the recent/relevant news only.\n",
    "# googlenews.setTimeRange('07/28/2020', normalize_date(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_news(company,search_date):\n",
    "    googlenews.setTimeRange(search_date, search_date)\n",
    "    googlenews.clear()\n",
    "    selected_companies_list = []\n",
    "    googlenews.search(str(company))\n",
    "    #change range for custom range. But don't include 1, because it is already included\n",
    "    #in the default option, and including it will duplicate results.\n",
    "\n",
    "    # for i in range(2,4):\n",
    "    #     googlenews.getpage(i)\n",
    "    googlenews.getpage()\n",
    "\n",
    "    # googlenews.getpage(2)\n",
    "    # googlenews.getpage(3)\n",
    "    # return googlenews.result()\n",
    "    for i in googlenews.result():\n",
    "        # filtered for Bloomberg and Yonhap News based on my intuition and experience \n",
    "        # realted to their reliabilities and accuracies.\n",
    "        # BELOW COMMENTS MADE AFTER RUNNING TRIALS\n",
    "        # after running the trials, the scores actually represented the stock price              trend when other medias were included. Limiting media to Bloomberg and                 Yonhap seems to provide better results for positive scores, and including              other including other medias seems to provide better results for negative              scores.\n",
    "        # if ((i.get('media') == 'Bloomberg') or (i.get('media') == 'Yonhap News')):\n",
    "           selected_companies_list.append(i)\n",
    "    return selected_companies_list\n",
    "\n",
    "# print(len(search_news('SK Innovation Bloomberg')))\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "stock_name = 'SK Innovation'\n",
    "stock_id = '096770'\n",
    "\n",
    "# search_date = normalize_date(now)\n",
    "daterange = pd.date_range('07/01/2020', normalize_date(now))\n",
    "\n",
    "\n",
    "def gather_news(stock_name,search_date):\n",
    "    news_no_filter = search_news(stock_name,search_date)\n",
    "    news_yonhap = search_news(stock_name+' Yonhap',search_date)\n",
    "    news_bloomberg = search_news(stock_name+' Bloomberg',search_date)\n",
    "    news_korea = search_news('South Korea Bloomberg',search_date)\n",
    "\n",
    "    combined_dates = []\n",
    "    combined_headlines = []\n",
    "\n",
    "    for i in news_no_filter:\n",
    "        # print(i.get('title'))\n",
    "        # remove brackets with irrelevant texts in title\n",
    "        new = re.sub(r'\\([^)]*\\) ', '', i.get('title'))\n",
    "        combined_headlines.append(new)\n",
    "        new = search_date\n",
    "        combined_dates.append(new)\n",
    "\n",
    "    for i in news_yonhap:\n",
    "        # print(i.get('title'))\n",
    "        # remove brackets with irrelevant texts in title\n",
    "        new = re.sub(r'\\([^)]*\\) ', '', i.get('title'))\n",
    "        combined_headlines.append(new)\n",
    "\n",
    "        combined_dates.append(search_date)\n",
    "\n",
    "    for i in news_bloomberg:\n",
    "        new = re.sub(r'\\([^)]*\\) ', '', i.get('title'))\n",
    "        combined_headlines.append(new)\n",
    "\n",
    "        combined_dates.append(search_date)\n",
    "\n",
    "    for i in news_korea:\n",
    "        new = re.sub(r'\\([^)]*\\) ', '', i.get('title'))\n",
    "        combined_headlines.append(new)\n",
    "\n",
    "        combined_dates.append(search_date)\n",
    "\n",
    "    # return combined_dates,combined_headlines\n",
    "    return combined_dates,combined_headlines\n",
    "\n",
    "\n",
    "# print(len(gather_news(stock_name)))\n",
    "# print(gather_news(stock_name))\n",
    "list_1=[]\n",
    "list_2=[]\n",
    "for single_date in daterange:\n",
    "    temp_dates,temp_headlines = gather_news(stock_name,normalize_date(single_date.date()))\n",
    "    list_1.extend(temp_dates)\n",
    "    list_2.extend(temp_headlines)\n",
    "    # print(temp_dates)\n",
    "    # print(temp_headlines)\n",
    "\n",
    "\n",
    "df = pandas.DataFrame(data={\"date\": list_1, \"headlines\": list_2})\n",
    "df.to_csv(\"./newsbydate.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_words=set(opinion_lexicon.positive())\n",
    "neg_words=set(opinion_lexicon.negative())\n",
    "\n",
    "# print(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "# tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# def sentiment_score(headline):\n",
    "#     score = 0\n",
    "#     for word in headline:\n",
    "#         if word in pos_words:\n",
    "#             score += 1\n",
    "#         elif word in neg_words:\n",
    "#             score -= 1\n",
    "#     return score\n",
    "\n",
    "# # This approach does not produce meaningful data that could be used. Therefore, an alternative method should be used. Upon reading Sohangir(2018), it is stated that Vader produced the highest score for Financial Sentiment Lexicon Analysis. Therefore, an alternative method using Vader should be considered.\n",
    "\n",
    "# for headline in gather_news(stock_name):\n",
    "#     tokenized_sentence = tokenizer.tokenize(headline)\n",
    "#     # print(tokenized_sentence)\n",
    "#     score = sentiment_score(tokenized_sentence)\n",
    "#     print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# # Prints individual headlines and their scores by date\n",
    "\n",
    "# df = pd.read_csv('file.csv')\n",
    "\n",
    "# del compound_scores\n",
    "# compound_scores = []\n",
    "\n",
    "# # filter the dataframe by date\n",
    "# for single_date in daterange:\n",
    "#     # temp_date = []\n",
    "#     norm_date = normalize_date(single_date.date())\n",
    "#     print('\\n')\n",
    "#     print(norm_date)\n",
    "#     for headline in (df[(df['date'] == str(norm_date))]).headlines:\n",
    "#         print(headline)\n",
    "#         scores = SentimentIntensityAnalyzer().polarity_scores(str(headline))\n",
    "#         for key in sorted(scores):\n",
    "#                 print('{0}: {1}, '.format(key, scores[key]), end='\\n')\n",
    "#                 if(key == 'compound' and scores[key]!= 0.0):\n",
    "#                     compound_scores.append(scores[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('newsbydate.csv')\n",
    "\n",
    "del compound_scores\n",
    "compound_scores = []\n",
    "\n",
    "score_date = []\n",
    "average_of_date = []\n",
    "# daterange = pd.date_range('08/10/2020', normalize_date(now))\n",
    "\n",
    "# filter the dataframe by date\n",
    "for single_date in daterange:\n",
    "    norm_date = normalize_date(single_date.date())\n",
    "    score_date.append(norm_date)\n",
    "    # print('\\n')\n",
    "    # print(norm_date)\n",
    "    for headline in (df[(df['date'] == str(norm_date))]).headlines:\n",
    "        # print(headline)\n",
    "        scores = SentimentIntensityAnalyzer().polarity_scores(str(headline))\n",
    "        for key in sorted(scores):\n",
    "                # print('{0}: {1}, '.format(key, scores[key]), end='\\n')\n",
    "                if(key == 'compound' and scores[key]!= 0.0):\n",
    "                    compound_scores.append(scores[key])\n",
    "\n",
    "                    # temp_date.append(scores[key])\n",
    "    if compound_scores:\n",
    "        # print(norm_date)\n",
    "        average_score = (sum(compound_scores)/len(compound_scores))\n",
    "        average_of_date.append(average_score)\n",
    "        # print(average_score)\n",
    "        \n",
    "df = pandas.DataFrame(data={\"date\": score_date, \"average_score\": average_of_date})\n",
    "df.to_csv(\"./averagescores.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "dataframe = web.DataReader(stock_id, 'naver', start='2020-07-01', end=dt.now()).reset_index()\n",
    "dataframe['Date'] = pd.to_datetime(dataframe['Date']).dt.date\n",
    "\n",
    "\n",
    "\n",
    "to_int = ['Open','High','Low','Close','Volume']\n",
    "\n",
    "dataframe[to_int] = dataframe[to_int].astype(str).astype(int)\n",
    "\n",
    "# Change is defined as the difference in the closing prices of neighbouring dates\n",
    "dataframe['Change'] = dataframe.Close.diff().fillna(0)\n",
    "dataframe = dataframe.iloc[:]\n",
    "dataframe.plot(x='Date',y='Change',figsize=(15,15))\n",
    "\n",
    "# resample dataframe to include missing weekend/holiday dates and fill the values with 0\n",
    "dataframe.set_index(dataframe.Date, inplace=True)\n",
    "dataframe.index = pd.to_datetime(dataframe.index)\n",
    "dataframe = dataframe.resample('D').sum().fillna(method='ffill')\n",
    "\n",
    "sentiment = pd.read_csv('averagescores.csv', parse_dates=['date'])\n",
    "sentiment.index = sentiment.date\n",
    "sentiment = sentiment.drop('date',axis=1)\n",
    "sentiment.plot(x='date',y='average_score',figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match starting and ending dates\n",
    "sentiment.iloc[1:]\n",
    "dataframe.iloc[:-1]\n",
    "\n",
    "# add change column to sentiment df\n",
    "sentiment['change'] = dataframe['Change']\n",
    "sentiment.replace(0, np.nan, inplace=True)\n",
    "sentiment = sentiment.fillna(method='ffill')\n",
    "sentiment = sentiment.fillna(method='bfill')\n",
    "\n",
    "# sentiment\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# score increases in a predictable pattern as test_size decreases indicating overfitting.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentiment.average_score,sentiment.change,test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 22.222%\n"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# Ys = scaler.fit_transform(pd.DataFrame(Y))\n",
    "\n",
    "\n",
    "X_train = X_train.values.reshape(-1,1)\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "\n",
    "X_test = X_test.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "\n",
    "X_train = np.nan_to_num(X_train)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "\n",
    "svc = LinearSVC(random_state=1)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
